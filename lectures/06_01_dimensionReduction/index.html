<!DOCTYPE html>
<html>
<head>
  <title>Dimension reduction</title>
  <meta charset="utf-8">
  <meta name="description" content="Dimension reduction">
  <meta name="author" content="Jeffrey Leek">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="../../libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="../../libraries/frameworks/io2012/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="../../libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="../../libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="../../libraries/frameworks/io2012/js/slides" 
    src="../../libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
    <link rel="stylesheet" href = "../../assets/css/custom.css">
<link rel="stylesheet" href = "../../assets/css/custom.css.BACKUP.546.css">
<link rel="stylesheet" href = "../../assets/css/custom.css.BASE.546.css">
<link rel="stylesheet" href = "../../assets/css/custom.css.LOCAL.546.css">
<link rel="stylesheet" href = "../../assets/css/custom.css.orig">
<link rel="stylesheet" href = "../../assets/css/custom.css.REMOTE.546.css">
<link rel="stylesheet" href = "../../assets/css/ribbons.css">

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
    <!-- END LOGO SLIDE -->
    

    <!-- TITLE SLIDE -->
    <!-- Should I move this to a Local Layout File? -->
    <slide class="title-slide segue nobackground">
      <aside class="gdbar">
        <img src="../../assets/img/bloomberg_shield.png">
      </aside>
      <hgroup class="auto-fadein">
        <h1>Dimension reduction</h1>
        <h2></h2>
        <p>Jeffrey Leek<br/>Johns Hopkins Bloomberg School of Public Health</p>
      </hgroup>
          </slide>

    <!-- SLIDES -->
      <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Pro Tip</h2>
  </hgroup>
  <article>
    <p>The person who comes to the meeting with the most hard data/work done wins. Most people (your instructor included) are so discombobulated/busy that they will show up with more opinions than facts. If you show up with solid, carefully considered evidence you are way ahead. You will go to a ton of meetings in your career. The way to not be disappointed is to be the most prepared person in the room.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Papers of the day</h2>
  </hgroup>
  <article>
    <p><a href="http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf">The PageRank Citation Ranking: Bringing order to the web</a></p>

<p>Abstract:</p>

<p><em>The importance of a Web page is an inherently sub jective matter, which depends on the
readers interests, knowledge and attitudes. But there is still much that can be said ob jectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages ob jectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to effciently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.</em></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Not a multibillion dollar company but</h2>
  </hgroup>
  <article>
    <p><img class="center" src="../../assets/img/svd.png" height=275></p>

<p><img class="center" src="../../assets/img/eigenstrat.png" height=275></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Matrix data</h2>
  </hgroup>
  <article>
    <pre><code class="r">set.seed(12345); par(mar=rep(0.2,4))
dataMatrix &lt;- matrix(rnorm(400),nrow=40)
image(1:10,1:40,t(dataMatrix)[,nrow(dataMatrix):1])
</code></pre>

<div class="rimage center"><img src="fig/randomData.png" title="plot of chunk randomData" alt="plot of chunk randomData" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Cluster the data</h2>
  </hgroup>
  <article>
    <pre><code class="r">par(mar=rep(0.2,4))
heatmap(dataMatrix)
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-1.png" title="plot of chunk unnamed-chunk-1" alt="plot of chunk unnamed-chunk-1" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>What if we add a pattern?</h2>
  </hgroup>
  <article>
    <pre><code class="r">set.seed(678910)
for(i in 1:40){
  # flip a coin
  coinFlip &lt;- rbinom(1,size=1,prob=0.5)
  # if coin is heads add a common pattern to that row
  if(coinFlip){
    dataMatrix[i,] &lt;- dataMatrix[i,] + rep(c(0,3),each=5)
  }
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>What if we add a pattern? - the data</h2>
  </hgroup>
  <article>
    <pre><code class="r">par(mar=rep(0.2,4))
image(1:10,1:40,t(dataMatrix)[,nrow(dataMatrix):1])
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-2.png" title="plot of chunk unnamed-chunk-2" alt="plot of chunk unnamed-chunk-2" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>What if we add a pattern? - the clustered data</h2>
  </hgroup>
  <article>
    <pre><code class="r">par(mar=rep(0.2,4))
heatmap(dataMatrix)
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-3.png" title="plot of chunk unnamed-chunk-3" alt="plot of chunk unnamed-chunk-3" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Patterns in rows and columns</h2>
  </hgroup>
  <article>
    <pre><code class="r">hh &lt;- hclust(dist(dataMatrix)); dataMatrixOrdered &lt;- dataMatrix[hh$order,]
par(mfrow=c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered),40:1,,xlab=&quot;Row Mean&quot;,ylab=&quot;Row&quot;,pch=19)
plot(colMeans(dataMatrixOrdered),xlab=&quot;Column&quot;,ylab=&quot;Column Mean&quot;,pch=19)
</code></pre>

<div class="rimage center"><img src="fig/oChunk.png" title="plot of chunk oChunk" alt="plot of chunk oChunk" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Related problems</h2>
  </hgroup>
  <article>
    <p>You have multivariate variables \(X_1,\ldots,X_n\) so \(X_1 = (X_{11},\ldots,X_{1m})\)</p>

<ul>
<li>Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible.</li>
<li>If you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data.</li>
</ul>

<p>The first goal is <font color="#330066">statistical</font> and the second goal is <font color="#993300">data compression</font>.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Same thing, now with more math</h2>
  </hgroup>
  <article>
    <p>Suppose we have \(n\) measurements on each of \(m\) variables \(X_j\), \(j=1,\ldots,n\). There are (at least) two equivalent ways to set up the problem</p>

<ul>
<li>Produce a derived (and small) set of uncorrelated variables \(Z_k = \alpha_k X\), \(k=1,2,\ldots,q < m\) that are linear combinations of the original variables, and that explain most of the variation in the original data</li>
<li>Approximate the \(n \times m\) matrix \(X\) by the best rank-\(q\) matrix \(\hat{X}_{(q)}\). </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Related solutions - PCA/SVD</h2>
  </hgroup>
  <article>
    <p><strong>SVD</strong></p>

<p>If \(X\) is a matrix with each variable in a column and each observation in a row then the SVD is a &quot;matrix decomposition&quot;</p>

<p>\[ X = UDV^T\]</p>

<p>where the columns of \(U\) are orthogonal (left singular vectors), the columns of \(V\) are orthogonal (right singular vectors) and \(D\) is a diagonal matrix (singular values). </p>

<p><strong>PCA</strong></p>

<p>The principal components are equal to the right singular values if you first scale (subtract the mean, divide by the standard deviation) the variables.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>The idea behind PCA</h2>
  </hgroup>
  <article>
    <p>If \(X\) is a random vector with mean \(0\) and covariance matrix \(\Sigma\) then the variance of the linear combination \(Z = \alpha^T X\) is given by:</p>

<p>\[{\rm Var}(Z) = \alpha^T\Sigma\alpha\]</p>

<p>We are seeking an \(\alpha\) such that \({\rm Var}(Z)\) is large; clearly we must impose a scale restriction on \(\alpha\). This leads to the principal-component criterion:</p>

<p>\(\max_{\alpha} \alpha^T\Sigma\alpha\) subject to \(||\alpha|| = 1\)</p>

<p>The solution \(\alpha\) is the largest eigenvector of \(\Sigma\):</p>

<p>\[\Sigma \alpha = d^2 \alpha\]</p>

<p>and \({\rm Var}(Z) = {\rm Var}(\alpha^T X) = d^2\).</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>PCA derived variables</h2>
  </hgroup>
  <article>
    <p><img class="center" src="../../assets/img/pcs1.png" height=400></p>

<p>\(Z_1 = \alpha_1^T X\) is the projection of the data onto the longest direction, and has the largest variance amongst all such normalized projections. \(\alpha_1\) is the largest eigenvalue of \(\hat{\Sigma}\), the sample covariance matrix of \(X\). \(Z_2\) and \(\alpha_2\) correspond to the second-largest eigenvector.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Singular value decomposition</h2>
  </hgroup>
  <article>
    <p>For any \(n \times m\) matrix \(X\) (assume \(n > m\)):
\[X = UDV^T\]
is the SVD of \(X\) where</p>

<ul>
<li>\(U\) is \(n \times m\) orthogonal, the left singular vectors</li>
<li>\(V\) is \(m \times m\) orthogonal, the right singular vectors</li>
<li>\(D\) is diagonal with \(d_1 \geq d_2 \geq \cdots \geq d_m \geq 0\), the singular </li>
</ul>

<p>The SVD always exists, and is unique (up to signs and ties). </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>SVD is the best rank \(q\) approximation</h2>
  </hgroup>
  <article>
    <p>Let \(D_q\) be \(D\) but all but the first \(q\) diagonal elements set to zero. Then \(\hat{X}_q = UD_qV^T\) solves:</p>

<p>\[\min_{{\rm rank}(\hat{X}_q) = q} || X - \hat{X}_q||_F\]</p>

<p>here \(||\cdot||_F\) is the Frobenius norm: \(||X||_F = \sqrt{\sum_{i,j} x_{ij}^2}\) \(= \sqrt{{\rm tr}(X^TX)}\)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Some important mathematical facts</h2>
  </hgroup>
  <article>
    <p>\[X = UDV^T = \sum_{j=1}^r \sigma_j u_j v_j^T\] </p>

<p>where \(r\) is the rank of \(X\). Since \(\min_{r(Z) = k} ||X-Z||_F\) is</p>

<p>\[Z = \hat{X}_k = \sum_{j=1}^k \sigma_j u_j v_j^T\]</p>

<p>the truncation error is:</p>

<p>\[|| X - \hat{X}_k||^2 = \sum_{j=k+1}^r \sigma^2_j\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>SVD dimensions in pictures</h2>
  </hgroup>
  <article>
    <p><img class="center" src="../../assets/img/svddim1.png" height=300></p>

<p><img class="center" src="../../assets/img/svddim2.png" height=150></p>

<p><a href="https://dl.dropboxusercontent.com/u/7710864/jhsph753/lectures/vadim.pdf">https://dl.dropboxusercontent.com/u/7710864/jhsph753/lectures/vadim.pdf</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>SVD and residuals</h2>
  </hgroup>
  <article>
    <p>\(\hat{X}_k = \sum_{j=1}^k \sigma u_jv_j^T\) and \(R = \sum_{k+1}^n \sigma u_j v_j^T\) for \(k=\{1,3,9,18,36,72,144\}\)</p>

<p><img class="center" src="../../assets/img/svdresid2.png" height=150></p>

<p><img class="center" src="../../assets/img/svdresid1.png" height=300></p>

<p><a href="https://dl.dropboxusercontent.com/u/7710864/jhsph753/lectures/vadim.pdf">https://dl.dropboxusercontent.com/u/7710864/jhsph753/lectures/vadim.pdf</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>Data compression from SVD</h2>
  </hgroup>
  <article>
    <ul>
<li>Instead of storing \(\hat{X}_k\) as \(m \times n\) matrix store \(k \times m + k \times n + k = k \times (m+n+1)\) values.</li>
<li>Since often \(r(X) = \min(m,n)\)</li>
<li>Typically \(k << r(X)\)</li>
<li>Typically \(k \times (m + n + 1) << mn\) </li>
<li>In this sense matrix reduction is equivalent to data compression</li>
</ul>

<p><a href="https://dl.dropboxusercontent.com/u/7710864/jhsph753/lectures/vadim.pdf">https://dl.dropboxusercontent.com/u/7710864/jhsph753/lectures/vadim.pdf</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>Important fact</h2>
  </hgroup>
  <article>
    <p>\[ X^TX = VDU^T UDV^T = VD^2V^T\]
\[ XX^T  = UDV^T VDU^T = UD^2U^T\]</p>

<p>\(X^TX\) and \(XX^T\) are symmetric matrices. </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>Spectral theorem</h2>
  </hgroup>
  <article>
    <p>Every symmetric matrix \(\Sigma\) can be decomposed</p>

<p>\[ \Sigma = W \Lambda W^T\] </p>

<ul>
<li>\(W\) are orthogonal eigenvectors</li>
<li>\(\Lambda\) are a diagonal matrix of real valued eigenvectors</li>
<li>\(CW = W\Lambda\) so \(Cw_j = \lambda_i w_j\) (eigenvector definition)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h2>Relationship to PCA</h2>
  </hgroup>
  <article>
    <p>If \(X\) is centered (column means zero). Then </p>

<p>\[C = \frac{1}{n}\sum_i x_i x_i^T = \frac{1}{n}XX^T\]</p>

<p>is an estimator of covariance. \(C\) is symmetric and we are looking for the largest eigenvalue. Recall that</p>

<p>\[C = W\Lambda W = VD^{2}V^T\]</p>

<p>For \(X\) is centered (column means zero)</p>

<ul>
<li>The columns of \(V\) are the principal components, </li>
<li>\(Z_j = U_j d_j\) are the scores</li>
<li>\(\frac{d_i^2}{\sum_{j=1}^m d_j^2}\) is the ``fraction of variation explained&#39;&#39; by the $i$th singular vector</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>Linear-Bilinear regression</h2>
  </hgroup>
  <article>
    <p>Gabriel 1978 JRSSB showed how to do this with covariates. Suppose we want to minimize:</p>

<p>\[\min_{{\rm rank}(\hat{X}_q) = q} || X - \beta Z - \hat{X}_q||_F\]</p>

<p>We can do this in three steps:</p>

<ul>
<li>Calculate the least squares estimator \(\hat{\beta}\) minimizing  \(|| X - \beta Z |_F\)</li>
<li>Calculate the residuals \(R = X - \hat{\beta}Z\)</li>
<li>Take the singular value decomposition of \(R = UDV^T\). Set all but the first \(q\) diagonal elements of \(D\) to zero. </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>A blessing of dimensionality</h2>
  </hgroup>
  <article>
    <p><img class="center" src="../../assets/img/blessdim.png" height=500></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>A property of imbalanced dimensions</h2>
  </hgroup>
  <article>
    <p><img class="center" src="../../assets/img/leekstoreythm1.png" height=300></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>Components of the SVD - \(u\) and \(v\)</h2>
  </hgroup>
  <article>
    <pre><code class="r">svd1 &lt;- svd(scale(dataMatrixOrdered))
par(mfrow=c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(svd1$u[,1],40:1,,xlab=&quot;Row&quot;,ylab=&quot;First left singular vector&quot;,pch=19)
plot(svd1$v[,1],xlab=&quot;Column&quot;,ylab=&quot;First right singular vector&quot;,pch=19)
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-4.png" title="plot of chunk unnamed-chunk-4" alt="plot of chunk unnamed-chunk-4" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h2>Components of the SVD - Variance explained</h2>
  </hgroup>
  <article>
    <pre><code class="r">par(mfrow=c(1,2))
plot(svd1$d,xlab=&quot;Column&quot;,ylab=&quot;Singular value&quot;,pch=19)
plot(svd1$d^2/sum(svd1$d^2),xlab=&quot;Column&quot;,ylab=&quot;Prop. of variance explained&quot;,pch=19)
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-5.png" title="plot of chunk unnamed-chunk-5" alt="plot of chunk unnamed-chunk-5" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-29" style="background:;">
  <hgroup>
    <h2>Relationship to principal components</h2>
  </hgroup>
  <article>
    <pre><code class="r">svd1 &lt;- svd(scale(dataMatrixOrdered))
pca1 &lt;- prcomp(dataMatrixOrdered,scale=TRUE)
plot(pca1$rotation[,1],svd1$v[,1],pch=19,xlab=&quot;Principal Component 1&quot;,ylab=&quot;Right Singular Vector 1&quot;)
abline(c(0,1))
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-6.png" title="plot of chunk unnamed-chunk-6" alt="plot of chunk unnamed-chunk-6" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-30" style="background:;">
  <hgroup>
    <h2>Components of the SVD - variance explained</h2>
  </hgroup>
  <article>
    <pre><code class="r">constantMatrix &lt;- dataMatrixOrdered*0
for(i in 1:dim(dataMatrixOrdered)[1]){constantMatrix[i,] &lt;- rep(c(0,1),each=5)}
svd1 &lt;- svd(constantMatrix)
par(mfrow=c(1,3))
image(t(constantMatrix)[,nrow(constantMatrix):1])
plot(svd1$d,xlab=&quot;Column&quot;,ylab=&quot;Singular value&quot;,pch=19)
plot(svd1$d^2/sum(svd1$d^2),xlab=&quot;Column&quot;,ylab=&quot;Prop. of variance explained&quot;,pch=19)
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-7.png" title="plot of chunk unnamed-chunk-7" alt="plot of chunk unnamed-chunk-7" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-31" style="background:;">
  <hgroup>
    <h2>What if we add a second pattern?</h2>
  </hgroup>
  <article>
    <pre><code class="r">set.seed(678910)
for(i in 1:40){
  # flip a coin
  coinFlip1 &lt;- rbinom(1,size=1,prob=0.5)
  coinFlip2 &lt;- rbinom(1,size=1,prob=0.5)
  # if coin is heads add a common pattern to that row
  if(coinFlip1){
    dataMatrix[i,] &lt;- dataMatrix[i,] + rep(c(0,5),each=5)
  }
  if(coinFlip2){
    dataMatrix[i,] &lt;- dataMatrix[i,] + rep(c(0,5),5)
  }
}
hh &lt;- hclust(dist(dataMatrix)); dataMatrixOrdered &lt;- dataMatrix[hh$order,]
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-32" style="background:;">
  <hgroup>
    <h2>Singular value decomposition - true patterns</h2>
  </hgroup>
  <article>
    <pre><code class="r">svd2 &lt;- svd(scale(dataMatrixOrdered))
par(mfrow=c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(rep(c(0,1),each=5),pch=19,xlab=&quot;Column&quot;,ylab=&quot;Pattern 1&quot;)
plot(rep(c(0,1),5),pch=19,xlab=&quot;Column&quot;,ylab=&quot;Pattern 2&quot;)
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-8.png" title="plot of chunk unnamed-chunk-8" alt="plot of chunk unnamed-chunk-8" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-33" style="background:;">
  <hgroup>
    <h2>\(v\) and patterns of variance in rows</h2>
  </hgroup>
  <article>
    <pre><code class="r">svd2 &lt;- svd(scale(dataMatrixOrdered))
par(mfrow=c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(svd2$v[,1],pch=19,xlab=&quot;Column&quot;,ylab=&quot;First right singular vector&quot;)
plot(svd2$v[,2],pch=19,xlab=&quot;Column&quot;,ylab=&quot;Second right singular vector&quot;)
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-9.png" title="plot of chunk unnamed-chunk-9" alt="plot of chunk unnamed-chunk-9" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-34" style="background:;">
  <hgroup>
    <h2>\(d\) and variance explained (scree plot)</h2>
  </hgroup>
  <article>
    <pre><code class="r">svd1 &lt;- svd(scale(dataMatrixOrdered))
par(mfrow=c(1,2))
plot(svd1$d,xlab=&quot;Column&quot;,ylab=&quot;Singular value&quot;,pch=19)
plot(svd1$d^2/sum(svd1$d^2),xlab=&quot;Column&quot;,ylab=&quot;Percent of variance explained&quot;,pch=19)
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-10.png" title="plot of chunk unnamed-chunk-10" alt="plot of chunk unnamed-chunk-10" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-35" style="background:;">
  <hgroup>
    <h2>A pathological case</h2>
  </hgroup>
  <article>
    <pre><code class="r">dataMatrix &lt;- matrix(rnorm(400),nrow=40)
dataMatrix[1,] = 50*rep(c(0,1),each=5)
ss = svd(dataMatrix - rowMeans(dataMatrix)); plot(ss$v[,1])
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-11.png" title="plot of chunk unnamed-chunk-11" alt="plot of chunk unnamed-chunk-11" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-36" style="background:;">
  <hgroup>
    <h2>Choosing the number of components</h2>
  </hgroup>
  <article>
    <p>If \(m\) is large:
<img class="center" src="../../assets/img/acsv.png" height=150></p>

<p>Otherwise:</p>

<ul>
<li>Calculate observed statistics \(s_i = \frac{d_i^2}{\sum_{j=1}^m d_j^2}\), \(i=1,\ldots,m\)</li>
<li>Permute each row or column of the data separately to get null matrices \(X^{0b}\), \(b=1,\ldots,B\)</li>
<li>Recalculate the SVD and get null statistics \(s_i^0 = \frac{d_i^{02}}{\sum_{j=1}^m d_j^{02}}\), \(i=1,\ldots,m\)</li>
<li>Calculate \(p\)-values for each component:
\[ p_i = \frac{1+\sum_{b=1}^B I(s_i^0 > s_i)}{B+1}\]</li>
</ul>

<p>Buja, A. and Eyuboglu, N. (1992). Remarks on parallel analysis.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-37" style="background:;">
  <hgroup>
    <h2>Missing values</h2>
  </hgroup>
  <article>
    <pre><code class="r">dataMatrix2 &lt;- dataMatrixOrdered
## Randomly insert some missing data
dataMatrix2[sample(1:100,size=40,replace=FALSE)] &lt;- NA
svd1 &lt;- svd(scale(dataMatrix2))  ## Doesn&#39;t work!
</code></pre>

<pre><code>Error: infinite or missing values in &#39;x&#39;
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-38" style="background:;">
  <hgroup>
    <h2>Imputing {impute}</h2>
  </hgroup>
  <article>
    <pre><code class="r">library(impute)  ## Available from http://bioconductor.org
dataMatrix2 &lt;- dataMatrixOrdered
dataMatrix2[sample(1:100,size=40,replace=FALSE)] &lt;- NA
dataMatrix2 &lt;- impute.knn(dataMatrix2)$data
svd1 &lt;- svd(scale(dataMatrixOrdered)); svd2 &lt;- svd(scale(dataMatrix2))
par(mfrow=c(1,2)); plot(svd1$v[,1],pch=19); plot(svd2$v[,1],pch=19)
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-13.png" title="plot of chunk unnamed-chunk-13" alt="plot of chunk unnamed-chunk-13" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-39" style="background:;">
  <hgroup>
    <h2>Face example</h2>
  </hgroup>
  <article>
    <!-- ## source("http://dl.dropbox.com/u/7710864/courseraPublic/myplclust.R") -->

<pre><code class="r">load(&quot;data/face.rda&quot;)
image(t(faceData)[,nrow(faceData):1])
</code></pre>

<div class="rimage center"><img src="fig/loadFaceData_.png" title="plot of chunk loadFaceData " alt="plot of chunk loadFaceData " class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-40" style="background:;">
  <hgroup>
    <h2>Face example - variance explained</h2>
  </hgroup>
  <article>
    <pre><code class="r">svd1 &lt;- svd(scale(faceData))
plot(svd1$d^2/sum(svd1$d^2),pch=19,xlab=&quot;Singular vector&quot;,ylab=&quot;Variance explained&quot;)
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-14.png" title="plot of chunk unnamed-chunk-14" alt="plot of chunk unnamed-chunk-14" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-41" style="background:;">
  <hgroup>
    <h2>Face example - create approximations</h2>
  </hgroup>
  <article>
    <pre><code class="r">
svd1 &lt;- svd(scale(faceData))
## Note that %*% is matrix multiplication

# Here svd1$d[1] is a constant
approx1 &lt;- svd1$u[,1] %*% t(svd1$v[,1]) * svd1$d[1]

# In these examples we need to make the diagonal matrix out of d
approx5 &lt;- svd1$u[,1:5] %*% diag(svd1$d[1:5])%*% t(svd1$v[,1:5]) 
approx10 &lt;- svd1$u[,1:10] %*% diag(svd1$d[1:10])%*% t(svd1$v[,1:10]) 
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-42" style="background:;">
  <hgroup>
    <h2>Face example - plot approximations</h2>
  </hgroup>
  <article>
    <pre><code class="r">par(mfrow=c(1,4))
image(t(approx1)[,nrow(approx1):1], main = &quot;(a)&quot;)
image(t(approx5)[,nrow(approx5):1], main = &quot;(b)&quot;)
image(t(approx10)[,nrow(approx10):1], main = &quot;(c)&quot;)
image(t(faceData)[,nrow(faceData):1], main = &quot;(d)&quot;)  ## Original data
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-15.png" title="plot of chunk unnamed-chunk-15" alt="plot of chunk unnamed-chunk-15" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-43" style="background:;">
  <hgroup>
    <h2>Digits example</h2>
  </hgroup>
  <article>
    <p><img class="center" src="../../assets/img/digits.png" height=400></p>

<p>130 threes, a subset of 638 such threes and part of the handwritten digit dataset used in the Elements of Statistical learning. Each three is a \(16\times 16\) greyscale image and the variables \(X_j\) \(j =1,\ldots 256\) are the grey scale values for each pixel. </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-44" style="background:;">
  <hgroup>
    <h2>Digits example</h2>
  </hgroup>
  <article>
    <p><img class="center" src="../../assets/img/digits2.png" height=400></p>

<p>Two-component model has the form:</p>

<p>\[\hat{f}(\lambda)  = \bar{X} + \lambda_1 v_1 + \lambda_2 v_2\]</p>

<p><img class="center" src="../../assets/img/equation.png" height=50></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-45" style="background:;">
  <hgroup>
    <h2>Example application: Pagerank</h2>
  </hgroup>
  <article>
    <p><img class="center" src="../../assets/img/pagerank.png" height=400></p>

<p><a href="http://statweb.stanford.edu/%7Etibs/ElemStatLearn/">http://statweb.stanford.edu/~tibs/ElemStatLearn/</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-46" style="background:;">
  <hgroup>
    <h2>Random surfer model</h2>
  </hgroup>
  <article>
    <ul>
<li>\(L_{ij} = 1\) if page \(j\) points to page \(i\)</li>
<li>\(c_j = \sum_{i=1}^N L_{ij}\) equal the number of pages pointed to by page \(j\)</li>
<li>Think about a &quot;random surfer&quot; who starts on a page and clicks a link at random</li>
<li>What ist he probability they end up on any specific page?</li>
<li>This is a <a href="http://en.wikipedia.org/wiki/Markov_chain">Markov Chain</a></li>
</ul>

<p>\[p_i = (1-d)/N + d \sum_{j=1}^N \left(\frac{L_{ij}}{c_j} p_j\right)\]</p>

<p><a href="http://statweb.stanford.edu/%7Etibs/ElemStatLearn/">http://statweb.stanford.edu/~tibs/ElemStatLearn/</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-47" style="background:;">
  <hgroup>
    <h2>Pagerank as an eigenvalue problem</h2>
  </hgroup>
  <article>
    <p>\[p = (1-d) e + d LD_c^{-1}p\]</p>

<p>where \(e\) is a vector of ones and \(D_c = diag(c)\) is a diagonal matrix with elements \(c_j\). If we assume \(e^Tp = N\) (so the average PageRank is 1) then:</p>

<p>\[p = [(1-d)ee^T/N+ dLD_c^{-1}]p\]
\[ = Ap\]</p>

<ul>
<li>\(A\) has largest eigenvalue equal to 1 (properties of Markov Chain)</li>
<li>So you can solve for the eigenvector using the <a href="http://en.wikipedia.org/wiki/Power_iteration">power method</a></li>
<li>\(p_k \leftarrow A p_{k-1}\) ; \(p_k \leftarrow N \frac{p_k}{e^Tp_k}\)</li>
</ul>

<p><a href="http://statweb.stanford.edu/%7Etibs/ElemStatLearn/">http://statweb.stanford.edu/~tibs/ElemStatLearn/</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-48" style="background:;">
  <hgroup>
    <h2>Back to the example</h2>
  </hgroup>
  <article>
    <p><img class="center" src="../../assets/img/pagerank.png" height=200></p>

<p>\[L = \left(\begin{array}{cccc} 0 & 0 & 1 & 0 \\ 1 & 0 & 0 & 0 \\ 1 & 1& 0 & 1 \\ 0 & 0 & 0& 0 \end{array}\right)\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-49" style="background:;">
  <hgroup>
    <h2>Doing PageRank in R</h2>
  </hgroup>
  <article>
    <pre><code class="r">L = matrix(c(0,0,1,0,1,0,0,0,1,1,0,1,0,0,0,0),byrow=T,nrow=4)
cc = colSums(L); d = 0.85; e = rep(1,4); N = 4
A = (1-d) * e %*% t(e) + d * L %*% solve(diag(cc))
Re(eigen(A)$values)
</code></pre>

<pre><code>[1]  1.450e+00 -4.250e-01 -4.250e-01  1.761e-17
</code></pre>

<pre><code class="r">phat = Re(eigen(A)$vectors[1,]); phat = (N* phat/(t(e) %*% phat))
phat
</code></pre>

<pre><code>[1]  1.185e+00  1.408e+00  1.408e+00 -7.284e-16
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-50" style="background:;">
  <hgroup>
    <h2>Notes and further resources</h2>
  </hgroup>
  <article>
    <ul>
<li>Scale matters</li>
<li>PC&#39;s/SV&#39;s may mix real patterns</li>
<li>Can be computationally intensive</li>
<li><a href="http://www.stat.cmu.edu/%7Ecshalizi/ADAfaEPoV/ADAfaEPoV.pdf">Advanced data analysis from an elementary point of view</a></li>
<li><a href="http://www-stat.stanford.edu/%7Etibs/ElemStatLearn/">Elements of statistical learning</a></li>
<li>Alternatives

<ul>
<li><a href="http://en.wikipedia.org/wiki/Factor_analysis">Factor analysis</a></li>
<li><a href="http://en.wikipedia.org/wiki/Independent_component_analysis">Independent components analysis</a></li>
<li><a href="http://en.wikipedia.org/wiki/Latent_semantic_analysis">Latent semantic analysis</a></li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>

  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
<!-- Grab CDN jQuery, fall back to local if offline -->
<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
<script>window.jQuery || document.write('<script src="../../libraries/widgets/quiz/js/jquery-1.7.min.js"><\/script>')</script>
<!-- Load Javascripts for Widgets -->
<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="../../libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="../../libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
</html>